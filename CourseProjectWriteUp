

**********************************************************************
*  First we need to load the needed libraries
**********************************************************************

    
library(ggplot2);library(caret)

#C:\tools\RStudio\resources\pml-training.csv
#C:\tools\RStudio\resources\pml-testing.csv

**********************************************************************
* Set the working directory and pull out the csv files into memory in R
**********************************************************************


setwd("C:/tools/RStudio/resources/")
training<-read.csv("pml-training.csv")
testing<-read.csv("pml-testing.csv")







**********************************************************************
*
*  First pass through I left the NA values but got an error when trying to create a predict model and 
* it errored due to the NA values.  Thus I had to go back and eliminate them.  I replaced the NA with 
* a a mean (average) value for that column
*  
**********************************************************************

#replace na with mean
data<-read.csv("pml-training.csv")
for(i in 1:ncol(data)){
  data[is.na(data[,i]), i] <- mean(data[,i], na.rm = TRUE)
}

training <- data
**********************************************************************
*  
* In looking at the first few files in the dataset I found they would not really be useful 
* for helping to predict which class (A, B, C, etc.) of movement it was in so I eliminated them by nulling them.
*
**********************************************************************
# Remove first few columns that can't help with prediction
training[1:6] <- list(NULL) 

# do same for testing values
data2<-read.csv("pml-testing.csv")
for(i in 1:ncol(data2)){
  data2[is.na(data2[,i]), i] <- mean(data2[,i], na.rm = TRUE)
}

testing <- data


testing[1:6] <- list(NULL) 


**********************************************************************
* There seemed to be way too many fields in the data so I thought of reducing them somehow.
* In the Caret package's pre-processing help section (http://topepo.github.io/caret/preprocess.html)
* they refer to near Zero Variance predictors that helps identify predictors that are not useful or 
* can mess up the model.   This function gives the Frequency Ratio and Percent of Unique Values
**********************************************************************

#Check for near zero values
nzv <- nearZeroVar(training, saveMetrics= TRUE)
nzv[nzv$nzv,][1:99,]
#eliminate the near zero values 

**********************************************************************
*  Identify the near zero values and eliminate those fields from the datasets
**********************************************************************

nzv <- nearZeroVar(training)
filteredTraining <- training[,-nzv]
filteredTesting <- testing[,-nzv]
dim(training)
dim(filteredTraining)


**********************************************************************
*  I had a ton of performance problems with the datasets so I had to reduce it down
* to a very small sub-sample in order for it to work in a reasonable amount of time
**********************************************************************
#train the model
inTrain <- createDataPartition(y=filteredTraining$classe,p=0.95, list=FALSE)
intraining <- filteredTraining[inTrain,]
intesting <- filteredTraining[-inTrain,]

**********************************************************************
*  I created these folds as an attempt to get smaller slices but ended up not
* using them.
**********************************************************************
set.seed(32323)
folds <- createFolds(y=filteredTraining$classe,k=20,list=TRUE,returnTrain=TRUE)
sapply(folds,length)


#modFit <- train(classe~ .,data=filteredTraining,method="rf",prox=TRUE)
#modFit <- train(classe~ .,data=intesting,method="rf",prox=TRUE)
**********************************************************************
*  I used the Random Forests method
**********************************************************************
modFit <- train(classe~ .,data=folds[2],method="rf",prox=TRUE)

#predit


pred <- predict(modFit,intesting ); 

#get confusion matrix
cm <- confusionMatrix (pred, intesting$classe)

cm <- confusionMatrix (pred, intraining$classe)

**********************************************************************
*  I used the model against the test set and got a very high mark in the 
* system so I'm happy :-)
**********************************************************************

predict(modFit, filteredTesting, type="raw")
myprediction <- predict(modFit, filteredTesting, type="raw")




**********************************************************************
*  Out of sample error to be and estimate the error appropriately with cross-validation I 
* will show below ...
**********************************************************************
> modFit
Random Forest 

979 samples
 53 predictor
  5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 979, 979, 979, 979, 979, 979, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD  
   2    0.8844426  0.8534636  0.01455352   0.01848796
  27    0.8949152  0.8668461  0.01917958   0.02447591
  53    0.8796827  0.8475726  0.02358724   0.02999586

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 


**********************************************************************
*  Based on the model training information above I believe the
* sample error rate to be 88% 
**********************************************************************
